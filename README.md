# Restaurant Knowledgebase Chatbot

A Retrieval-Augmented Generation (RAG)-based chatbot that intelligently answers detailed restaurant-related questions by scraping real data, processing it, and retrieving the most relevant information.

---

## Project Overview

This project helps users get answers to restaurant-specific queries such as:

- "Which restaurant has the best vegetarian options?"
- "List all restaurants with dishes under 100 rupees."
- "Which restaurant serves the spiciest food?"
- "Which restaurants are offering sweets?"
- "Compare spice levels between two restaurants."

It is built using:

- Custom web scrapers
- Text embeddings using Hugging Face MiniLM models
- FAISS for fast similarity search
- Streamlit for a simple chatbot interface
- Local CPU or GPU support for embeddings

---

## Resources Used

- **Eatsure.com**: Used for scraping real restaurant data (menus, prices, categories, tags, etc.)
- **Hugging Face**: Pretrained MiniLM model (`sentence-transformers/all-MiniLM-L6-v2`) for generating embeddings.
- **FAISS**: Facebook AI Similarity Search library to store and retrieve embeddings.
- **Streamlit**: Used for building the web-based chatbot frontend.
- **Python Libraries**: `BeautifulSoup4`, `pandas`, `numpy`, `pickle`, `sentence-transformers`, `streamlit`, etc.

---

## Internal Flow of the Code

Here’s how the entire system works internally:

1. **Web Scraping (src/main.py and src/scraper/restaurant_scraper.py)**
    - Scrapes restaurant data like names, dishes, categories, prices, and other details from **Eatsure.com**.
    - Saves the raw data into `src/output/raw_extracted_data.json`.

2. **Preprocessing and Indexing (src/preprocess_and_index.py)**
    - Cleans and structures important fields like:
      - Price
      - Spiciness
      - Sweetness
      - Dish Type (Veg/Non-Veg)
      - Restaurant Ratings
    - Flattens the fields into readable text.
    - Generates **dense text embeddings** using the Hugging Face MiniLM model.
    - Stores:
      - Processed chunks into `processed_chunks.json`
      - FAISS vector index into `faiss_index.bin`
      - Metadata into `metadata.pkl`

3. **Retrieval and Chatbot (src/chatbot.py and streamlit_app.py)**
    - When a user asks a question:
      - The query is embedded.
      - Top 3 relevant text chunks are retrieved from FAISS.
      - An LLM generates a final answer based on the retrieved information.
    - Answers are shown to users in the Streamlit chat UI.

---

## Flow

User -> Streamlit -> Chatbot -> FAISS -> Retrieved Chunks -> LLM -> Final Answer

## Folder Structure

```
python-scraper-project/
│   .env
│   .gitignore
│   README.md
│   requirements.txt
│   streamlit_app.py
│
└───src/
    │   main.py  # Potentially unused or for other main scripts
    │   update_sites_to_fetch.py # Script to update config/sites.json from sitemap
    │   __init__.py # Makes 'src' a package
    │
    ├───chatbot/
    │   │   chatbot.py # Core chatbot logic (retrieval, generation)
    │   │   system_prompt.txt # Prompt for the LLM
    │   │   __init__.py
    │
    ├───config/
    │   │   sites.json # List of restaurant URLs to scrape
    │
    ├───output/ # Generated data files (should be in .gitignore except maybe defaults)
    │   │   faiss_index.bin # FAISS vector index
    │   │   knowledge_base.json # Intermediate structured data (optional)
    │   │   metadata.pkl # Metadata for FAISS vectors
    │   │   processed_chunks.json # Text chunks used for embeddings
    │   │   raw_extracted_data.json # Raw scraped data
    │
    ├───preprocessing/
    │   │   preprocess_and_index.py # Script for cleaning, processing, embedding, indexing
    │   │   __init__.py
    │
    ├───raw_data/
    │   │   extract_raw_data.py # Script to orchestrate scraping
    │   │   __init__.py
    │
    ├───scraper/
    │   │   restaurant_scraper.py # Class/functions for scraping a single URL
    │   │   __init__.py
    │
    └───utils/
        │   constants.py # Project-wide constants (e.g., MAX_RESTAURANTS_TO_FETCH)
        │   utils.py # Common utility functions
        │   __init__.py

# Note: __pycache__ directories are automatically generated by Python and usually excluded via .gitignore
```

---

## Installation

1. Clone the repository:

    ```bash
    git clone https://github.com/your-username/python-scraper-project.git
    cd python-scraper-project
    ```

2. Create a virtual environment and activate it:

    ```bash
    python -m venv venv
    # Windows:
    venv\Scripts\activate
    # Linux/macOS:
    source venv/bin/activate
    ```

3. Install all requirements:

    ```bash
    pip install -r requirements.txt
    ```

4. **Environment Variables:**
    - Create a `.env` file in the project root directory (`python-scraper-project/.env`).
    - Add your Google API key to the `.env` file:
      ```
      GOOGLE_API_KEY=YOUR_ACTUAL_API_KEY
      ```
    - **Important:** Add `.env` to your `.gitignore` file to avoid committing your API key.

---

## Running the Application

Ensure your virtual environment is activated before running any commands. Follow these steps to run the data processing pipeline and the chatbot application:

   a. **Update Site List:** Fetch the latest restaurant URLs from the sitemap and update `src/config/sites.json`.
      ```bash
      python -m src.update_sites_to_fetch
      ```

   b. **Extract Raw Data:** Scrape data from the URLs listed in `src/config/sites.json` and save it to `src/output/raw_extracted_data.json`.
      ```bash
      python -m src.raw_data.extract_raw_data
      ```

   c. **Preprocess and Index:** Process the raw data, generate embeddings, create the FAISS index (`faiss_index.bin`), and save metadata (`metadata.pkl`) and processed chunks (`processed_chunks.json`) in the `src/output/` directory.
      ```bash
      python -m src.preprocessing.preprocess_and_index
      ```

   d. **Run the Chatbot:** Start the Streamlit web application.
      ```bash
      streamlit run streamlit_app.py
      ```

---


## Example Queries

- List all restaurants with dishes under 100 rupees.
- Which restaurant has the spiciest dishes?
- Which restaurants offer sweet dishes?
- Show best-rated pizza places.
- Compare spice levels between Restaurant A and Restaurant B.

---

## Tech Stack

- **Python 3.10+**
- **BeautifulSoup4** (scraping HTML pages)
- **Sentence-Transformers** (Hugging Face MiniLM model)
- **FAISS** (similarity search)
- **Streamlit** (frontend chat app)
- **Torch** (for embeddings if GPU used)
- **Pandas, Numpy, Pickle** (processing and utilities)

---

## Project Structure Details

- **`streamlit_app.py`**: The main entry point for the Streamlit web application.
- **`src/`**: Contains the core Python modules.
  - **`chatbot.py`**: Handles query embedding, FAISS retrieval, and interaction with the LLM (Gemini).
  - **`extract_raw_data.py`**: Script to orchestrate the scraping process using `restaurant_scraper.py`.
  - **`main.py`**: Potentially an older entry point or contains shared functions like `load_config`.
  - **`preprocess_and_index.py`**: Cleans data, extracts features, generates text embeddings, builds the FAISS index, and saves metadata.
  - **`update_sites_to_fetch.py`**: Script to update the list of target URLs in `sites.json`.
  - **`utils.py`**: Utility functions used across different modules.
  - **`config/sites.json`**: Configuration file listing the target restaurant URLs to scrape. This file determines which restaurants' data will be included in the knowledge base.
  - **`output/`**: Directory for generated data files.
    - `raw_extracted_data.json`: Raw data scraped from websites.
    - `knowledge_base.json`: Structured data extracted and enriched during preprocessing (might be an alternative or intermediate format).
    - `processed_chunks.json`: Text chunks prepared for embedding.
    - `faiss_index.bin`: The FAISS vector index file.
    - `metadata.pkl`: Metadata corresponding to the vectors in the FAISS index.
  - **`scraper/restaurant_scraper.py`**: Contains the `RestaurantScraper` class responsible for the actual web scraping logic for a single restaurant URL.
  - **`constants.py`**: Project-wide constants (e.g., `MAX_RESTAURANTS_TO_FETCH`).
  - **`.env`**: Stores environment variables like API keys (should be in `.gitignore`).
  - **`requirements.txt`**: Lists Python package dependencies.
  - **`Makefile`**: Defines commands for building the data pipeline and running the app.

---

## Acknowledgements

- Hugging Face for open-source embedding models
- Facebook Research for FAISS
- Streamlit for enabling easy chatbot development

---
